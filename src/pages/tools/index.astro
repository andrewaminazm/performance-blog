---
import Layout from '../../layouts/Layout.astro';
---
<Layout title="Tools" description="Step-by-step guides for JMeter, k6, and Postman performance testing.">
  <header class="tools-header">
    <h1 class="tools-title">Performance testing tools</h1>
    <p class="tools-desc">Choose a tool below for full steps and details.</p>
  </header>

  <div class="tabs" id="tools-tabs">
    <div class="tab-list" role="tablist" aria-label="Tools">
      <button type="button" class="tab-btn active" role="tab" aria-selected="true" aria-controls="panel-jmeter" id="tab-jmeter" data-tab="jmeter">JMeter</button>
      <button type="button" class="tab-btn" role="tab" aria-selected="false" aria-controls="panel-k6" id="tab-k6" data-tab="k6">k6</button>
      <button type="button" class="tab-btn" role="tab" aria-selected="false" aria-controls="panel-postman" id="tab-postman" data-tab="postman">Postman</button>
    </div>

    <!-- JMeter panel -->
    <div id="panel-jmeter" class="tab-panel active" role="tabpanel" aria-labelledby="tab-jmeter">
      <article class="tool-content">
        <h2>Apache JMeter</h2>
        <p>Open-source load and performance testing tool. You build test plans in the GUI (or edit XML) using <strong>Thread Groups</strong>, <strong>Samplers</strong>, and <strong>Listeners</strong>. Supports HTTP, HTTPS, APIs, databases (JDBC), FTP, and more.</p>

        <h3>Prerequisites</h3>
        <ul>
          <li><strong>Java (JDK 11 or later)</strong> — JMeter is a Java application. Check with <code>java -version</code>.</li>
        </ul>

        <h3>Step 1: Install JMeter</h3>
        <ol>
          <li>Download the binary from <a href="https://jmeter.apache.org/download_jmeter.cgi" target="_blank" rel="noopener">jmeter.apache.org</a> (e.g. <code>apache-jmeter-5.6.3.zip</code>).</li>
          <li>Unzip to a folder (e.g. <code>C:\jmeter</code> or <code>/opt/jmeter</code>).</li>
          <li><strong>Windows:</strong> Run <code>bin\jmeter.bat</code>. <strong>Mac/Linux:</strong> Run <code>bin/jmeter</code>.</li>
          <li>The GUI will open with an empty Test Plan.</li>
        </ol>

        <h3>Step 2: Key concepts</h3>
        <ul>
          <li><strong>Test Plan</strong> — Root container; holds Thread Groups and shared config (user-defined variables, etc.).</li>
          <li><strong>Thread Group</strong> — Simulates users: number of threads (users), ramp-up time (seconds to start all threads), loop count (how many times each user runs the plan).</li>
          <li><strong>Samplers</strong> — The actual requests: HTTP Request, JDBC Request, etc. They sit under a Thread Group (or under a Controller).</li>
          <li><strong>Controllers</strong> — Logic: Loop Controller, If Controller, Transaction Controller (group samplers into one “transaction”), Simple Controller (grouping only).</li>
          <li><strong>Listeners</strong> — Show results: View Results Tree (detail), Summary Report (aggregate), Graph Results (charts). Use few listeners in large tests to save memory.</li>
          <li><strong>Assertions</strong> — Validate responses: Response Assertion (status, body), JSON Assertion, Duration Assertion.</li>
          <li><strong>Config elements</strong> — HTTP Header Manager, CSV Data Set Config (parameterization), User Defined Variables.</li>
        </ul>

        <h3>Step 3: Create your first test</h3>
        <ol>
          <li>Right-click <strong>Test Plan</strong> → Add → Threads (Users) → <strong>Thread Group</strong>.</li>
          <li>Set <strong>Number of Threads</strong> (e.g. 10), <strong>Ramp-up period</strong> (e.g. 5 seconds), <strong>Loop Count</strong> (e.g. 1 or “Infinite” with a duration later).</li>
          <li>Right-click <strong>Thread Group</strong> → Add → Sampler → <strong>HTTP Request</strong>.</li>
          <li>Set <strong>Server Name or IP</strong> (e.g. <code>httpbin.org</code>), <strong>Path</strong> (e.g. <code>/get</code>). Leave Protocol as blank if using HTTP.</li>
          <li>Right-click <strong>Thread Group</strong> → Add → Listener → <strong>View Results Tree</strong> (for debugging) and/or <strong>Summary Report</strong> (for aggregate stats).</li>
          <li>Click the green <strong>Start</strong> button. Watch the Listeners for results.</li>
        </ol>

        <h3>Step 4: Add assertions</h3>
        <ol>
          <li>Right-click your <strong>HTTP Request</strong> → Add → Assertions → <strong>Response Assertion</strong>.</li>
          <li>Add “Field to Test”: e.g. <strong>Response Code</strong>, pattern <code>200</code>.</li>
          <li>Optional: add <strong>Duration Assertion</strong> (max response time in ms).</li>
        </ol>

        <h3>Step 5: Parameterization (e.g. CSV)</h3>
        <ol>
          <li>Create a CSV with columns (e.g. <code>username</code>, <code>password</code>).</li>
          <li>Right-click <strong>Thread Group</strong> → Add → Config Element → <strong>CSV Data Set Config</strong>.</li>
          <li>Set <strong>Filename</strong> to your CSV path, <strong>Variable Names</strong> (comma-separated), delimiter if needed.</li>
          <li>In your HTTP Request (or other sampler), use <code>{'${username}'}</code>, <code>{'${password}'}</code> in fields.</li>
        </ol>

        <h3>Step 6: Run without GUI (recommended for load tests)</h3>
        <p>GUI mode consumes memory; for real load use CLI:</p>
        <pre><code>jmeter -n -t MyTestPlan.jmx -l results.jtl -e -o report</code></pre>
        <ul>
          <li><code>-n</code> = non-GUI mode.</li>
          <li><code>-t</code> = test plan file.</li>
          <li><code>-l</code> = results file (e.g. <code>results.jtl</code>).</li>
          <li><code>-e -o report</code> = generate HTML report in <code>report/</code> folder.</li>
        </ul>

        <h3>Step 7: Reporting</h3>
        <ul>
          <li><strong>Summary Report / Aggregate Report</strong> — Tables with throughput, average/min/max response time, error %.</li>
          <li><strong>HTML Report</strong> — Use <code>-e -o report</code> for charts and tables.</li>
          <li><strong>Backend Listener</strong> — Send metrics to InfluxDB (or similar) and visualize in Grafana for live dashboards.</li>
        </ul>

        <h3>How to pass parameters in JMeter</h3>
        <ul>
          <li><strong>Query parameters:</strong> In HTTP Request, use the <strong>Parameters</strong> table: add name (e.g. <code>page</code>) and value (e.g. <code>1</code> or <code>{'${page}'}</code> for a variable). JMeter appends <code>?page=1</code> to the path.</li>
          <li><strong>Path parameters:</strong> In <strong>Path</strong>, use variables: e.g. <code>/api/users/{'${userId}'}/orders</code>. Set <code>userId</code> via User Defined Variables or CSV Data Set Config.</li>
          <li><strong>Body (JSON/form):</strong> In HTTP Request, choose <strong>Body Data</strong> and type raw JSON. Use JMeter variables in the JSON (e.g. <code>{'${name}'}</code>, <code>{'${email}'}</code>). For form, use the Parameters table with method POST.</li>
          <li><strong>Data-driven (CSV):</strong> Add <strong>CSV Data Set Config</strong> (see Step 5). Reference columns as <code>{'${username}'}</code>, <code>{'${password}'}</code> in path, body, or headers.</li>
        </ul>

        <h3>How to pass token in JMeter</h3>
        <ul>
          <li><strong>Bearer token in header:</strong> Add <strong>HTTP Header Manager</strong> (right-click Thread Group or Request → Add → Config Element → HTTP Header Manager). Add header: Name <code>Authorization</code>, Value <code>Bearer {'${token}'}</code>. Define <code>token</code> in User Defined Variables or CSV.</li>
          <li><strong>API key in header:</strong> Same HTTP Header Manager: e.g. Name <code>X-API-Key</code>, Value <code>{'${apiKey}'}</code>.</li>
          <li><strong>Token from a previous request (e.g. login):</strong> Add a first HTTP Request (e.g. POST to <code>/auth/login</code>). Add <strong>JSON Extractor</strong> (Post Processors) to that request: Variable name <code>token</code>, JSON Path <code>$.access_token</code> (or your response path). Subsequent requests can use <code>Bearer {'${token}'}</code> in HTTP Header Manager.</li>
          <li><strong>User Defined Variables:</strong> Test Plan → Add → Config Element → User Defined Variables. Add <code>token</code> = <code>your_token_here</code> for a fixed value (or leave empty and pass via <code>-Jtoken=...</code> on CLI: <code>jmeter -n -t plan.jmx -Jtoken=abc123</code>).</li>
        </ul>

        <h3>Performance: what JMeter gives you</h3>
        <p>JMeter measures and reports <strong>throughput</strong> (requests/second), <strong>response time</strong> (min/avg/max and percentiles in the HTML report), and <strong>error rate</strong>. Threads = virtual users; ramp-up = how quickly they start; loop count + duration control test length. For performance runs always use <strong>non-GUI mode</strong> (<code>-n</code>) and limit Listeners to avoid extra memory and CPU on the load generator.</p>

        <h3>Performance scenarios in JMeter</h3>

        <h4>Scenario 1: Load test (baseline performance)</h4>
        <p><strong>Goal:</strong> Validate system under expected load. Simulate normal or target traffic.</p>
        <ul>
          <li>One <strong>Thread Group</strong>: e.g. 50 threads, ramp-up 60s, loop “Infinite” with <strong>Scheduler</strong> (duration 300s).</li>
          <li>Add <strong>Constant Timer</strong> under the request (e.g. 500ms) to simulate think time between requests.</li>
          <li>Run in CLI; check <strong>Summary Report</strong> or HTML report: throughput (TPS), average/p95 response time, error %.</li>
        </ul>

        <h4>Scenario 2: Stress test (find breaking point)</h4>
        <p><strong>Goal:</strong> Push beyond normal load to find when the system degrades or fails.</p>
        <ul>
          <li>Use multiple <strong>Thread Groups</strong> or one group with high thread count and short ramp-up (e.g. 200 threads, ramp-up 30s).</li>
          <li>Or use <strong>Stepping Thread Group</strong> (plugin) or <strong>Ultimate Thread Group</strong> to increase load in steps (e.g. +50 users every 2 minutes).</li>
          <li>Monitor when response time spikes or errors increase; that is your approximate capacity limit.</li>
        </ul>

        <h4>Scenario 3: Spike test</h4>
        <p><strong>Goal:</strong> Sudden traffic spike to see how the system reacts (e.g. flash sale).</p>
        <ul>
          <li>Short ramp-up, high threads: e.g. 100 threads, ramp-up 5s, run for 1–2 minutes.</li>
          <li>Or two Thread Groups: first at low load (e.g. 10 threads), then a second starting after 30s with 200 threads and 10s ramp-up.</li>
        </ul>

        <h4>Scenario 4: Endurance / soak test</h4>
        <p><strong>Goal:</strong> Run at moderate load for hours to find memory leaks, connection leaks, or slow degradation.</p>
        <ul>
          <li>Moderate threads (e.g. 20–50), long duration (e.g. 2–8 hours) via Scheduler or loop count.</li>
          <li>Monitor server memory and CPU over time; check for growing response times or errors toward the end.</li>
        </ul>

        <h4>Scenario 5: API load with JSON and headers</h4>
        <p><strong>Goal:</strong> Load test REST APIs (GET/POST/PUT) with JSON body and auth.</p>
        <ul>
          <li><strong>HTTP Request</strong>: Method POST, path <code>/api/orders</code>, Body Data = raw JSON.</li>
          <li>Add <strong>HTTP Header Manager</strong> (Config Element): <code>Content-Type: application/json</code>, <code>Authorization: Bearer {'${token}'}</code> (use variable).</li>
          <li>Use <strong>JSON Assertion</strong> or <strong>Response Assertion</strong> to validate status and body. Use <strong>Duration Assertion</strong> for performance (e.g. response &lt; 500ms).</li>
        </ul>

        <h4>Scenario 6: Mixed traffic (multiple user flows)</h4>
        <p><strong>Goal:</strong> Realistic mix: e.g. 70% browse, 20% search, 10% checkout.</p>
        <ul>
          <li>Create 3 Thread Groups with different thread counts (e.g. 70, 20, 10) or one Thread Group with <strong>Throughput Controller</strong> (percent) on each flow.</li>
          <li>Each flow has its own HTTP requests (or logic). Use <strong>Transaction Controller</strong> to group requests and get transaction-level response time.</li>
        </ul>

        <h3>Clarifications for performance</h3>
        <ul>
          <li><strong>Threads</strong> = concurrent virtual users. More threads = more load (if the server can handle it).</li>
          <li><strong>Ramp-up</strong> = time to start all threads. Too short can create a spike; too long delays reaching target load.</li>
          <li><strong>Loop count</strong> = how many times each thread runs the plan. “Infinite” + Scheduler duration = fixed test length.</li>
          <li><strong>Think time</strong> = <strong>Constant Timer</strong> or <strong>Uniform Random Timer</strong> between requests. Adds realism and reduces RPS per user.</li>
          <li><strong>Throughput (TPS)</strong> = transactions per second in Listeners. Aim for stable throughput and acceptable response time and error rate.</li>
        </ul>

        <h3>AI prompts and using AI with JMeter</h3>
        <p>JMeter has no built-in AI. Use <strong>ChatGPT</strong>, <strong>Claude</strong>, or similar to generate test plans, config snippets, or JMeter DSL (e.g. Ruby/Java DSL). Copy the prompts below and paste them into the AI; then apply the output in the JMeter GUI or save as <code>.jmx</code> / script.</p>

        <h4>Copy-paste prompts (JMeter)</h4>

        <p class="prompt-label"><strong>1. Test plan from scratch</strong></p>
        <pre class="prompt-copy"><code>Generate a JMeter test plan for load testing a REST API: 100 virtual users, 2-minute ramp-up, 5-minute duration. One GET request to https://api.example.com/items. Include a Summary Report listener and a Response Assertion for status 200. Output as JMeter DSL or step-by-step GUI instructions.</code></pre>

        <p class="prompt-label"><strong>2. Thread Group + timer</strong></p>
        <pre class="prompt-copy"><code>Give me a JMeter Thread Group config: 50 threads, 60s ramp-up, infinite loop with 300s duration. Add a Constant Timer of 500ms between requests. Explain each field.</code></pre>

        <p class="prompt-label"><strong>3. HTTP request with auth and JSON</strong></p>
        <pre class="prompt-copy"><code>{`Write JMeter steps for a POST request to /api/orders with JSON body and Bearer token in header. Include HTTP Header Manager and Body Data. Use variable \${token} for the token.`}</code></pre>

        <p class="prompt-label"><strong>4. Parameterization</strong></p>
        <pre class="prompt-copy"><code>Create a JMeter CSV Data Set Config: filename users.csv, variables username,email. How do I use these in an HTTP request path and body?</code></pre>

        <p class="prompt-label"><strong>5. Assertions and performance</strong></p>
        <pre class="prompt-copy"><code>Add a Response Assertion for status 200 and a Duration Assertion for max 500ms in JMeter. Where do I add them (under which element)?</code></pre>

        <p class="prompt-label"><strong>6. Stress test scenario</strong></p>
        <pre class="prompt-copy"><code>Describe how to do a JMeter stress test: start at 20 users, add 30 users every 2 minutes up to 200, then run 5 minutes. Use Stepping Thread Group or multiple Thread Groups with delayed start.</code></pre>

        <p class="prompt-label"><strong>7. Debug errors</strong></p>
        <pre class="prompt-copy"><code>My JMeter run shows 'Connection refused' / 'Timeout'. What should I check? (thread count, ramp-up, server, firewall, JMeter CLI vs GUI.)</code></pre>

        <h4>How to use the output</h4>
        <ul>
          <li>If the AI gives <strong>GUI steps</strong>, follow them in JMeter (Add → Thread Group, etc.).</li>
          <li>If it gives <strong>JMX XML</strong>, save as <code>.jmx</code> and open in JMeter (or merge into your plan).</li>
          <li>If it gives <strong>JMeter DSL</strong> (e.g. Ruby/Java), run the script to generate <code>.jmx</code> if your project uses that.</li>
          <li>Always <strong>review and adjust</strong> URLs, thread counts, and paths; then run a short test before a long load run.</li>
        </ul>

        <h3>Best practices</h3>
        <ul>
          <li>Disable or remove heavy Listeners (e.g. View Results Tree) when running large tests.</li>
          <li>Use CLI for any test with more than a few dozen threads.</li>
          <li>Use variables and CSV for realistic data; avoid hardcoding.</li>
          <li>Run tests from a machine close to the server (same network/region) to reduce network noise.</li>
        </ul>
      </article>
    </div>

    <!-- k6 panel -->
    <div id="panel-k6" class="tab-panel" role="tabpanel" aria-labelledby="tab-k6" hidden>
      <article class="tool-content">
        <h2>k6 (Grafana Labs)</h2>
        <p>Script-based <strong>performance</strong> and load testing tool. You write tests in <strong>JavaScript</strong> (ES6 modules); k6 runs them with configurable virtual users (VUs). Built-in metrics (latency percentiles, throughput, error rate) and <strong>thresholds</strong> make it ideal for performance gates in CI and for load, stress, and spike tests.</p>

        <h3>Prerequisites</h3>
        <ul>
          <li>No Java required. Single binary; optional: code editor for scripts.</li>
        </ul>

        <h3>Step 1: Install k6</h3>
        <ul>
          <li><strong>Windows:</strong> <code>winget install k6</code> or download from <a href="https://k6.io/docs/get-started/installation/" target="_blank" rel="noopener">k6.io</a>.</li>
          <li><strong>Mac:</strong> <code>brew install k6</code>.</li>
          <li><strong>Linux:</strong> See <a href="https://k6.io/docs/get-started/installation/" target="_blank" rel="noopener">k6.io/docs</a> (e.g. package manager or binary).</li>
          <li>Verify: <code>k6 version</code>.</li>
        </ul>

        <h3>Step 2: Key concepts</h3>
        <ul>
          <li><strong>VUs (virtual users)</strong> — Concurrent iterations of your script.</li>
          <li><strong>Iterations</strong> — One run of the default function; k6 schedules them according to stages and VUs.</li>
          <li><strong>options</strong> — Config object: <code>vus</code>, <code>duration</code>, <code>stages</code>, <code>thresholds</code>.</li>
          <li><strong>Stages</strong> — Ramp-up, steady, ramp-down (e.g. 0→10 VUs over 30s, hold 1m, then 0).</li>
          <li><strong>Checks</strong> — Per-request pass/fail (e.g. status 200); don’t stop the test.</li>
          <li><strong>Thresholds</strong> — Pass/fail criteria for the whole run (e.g. p95 &lt; 500ms, error rate &lt; 1%).</li>
        </ul>

        <h3>Step 3: First script</h3>
        <p>Create <code>script.js</code>:</p>
        <pre><code>{`import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  vus: 10,
  duration: '30s',
};

export default function () {
  const res = http.get('https://httpbin.org/get');
  check(res, { 'status is 200': (r) => r.status === 200 });
  sleep(1);
}`}</code></pre>
        <p>Run: <code>k6 run script.js</code>.</p>

        <h3>Step 4: Add stages (ramp-up / ramp-down)</h3>
        <pre><code>{`export const options = {
  stages: [
    { duration: '30s', target: 20 },  // ramp-up to 20 VUs
    { duration: '1m', target: 20 },   // stay at 20 VUs
    { duration: '20s', target: 0 },   // ramp-down to 0
  ],
};`}</code></pre>

        <h3>Step 5: Add thresholds</h3>
        <pre><code>{`export const options = {
  vus: 10,
  duration: '30s',
  thresholds: {
    http_req_duration: ['p(95)<500'],  // 95th percentile < 500ms
    http_req_failed: ['rate<0.01'],     // error rate < 1%
  },
};`}</code></pre>
        <p>If a threshold fails, k6 exits with a non-zero code (useful in CI).</p>

        <h3>Step 6: POST request and JSON</h3>
        <pre><code>{`import http from 'k6/http';

export default function () {
  const payload = JSON.stringify({ name: 'test', id: 1 });
  const params = { headers: { 'Content-Type': 'application/json' } };
  const res = http.post('https://httpbin.org/post', payload, params);
}`}</code></pre>

        <h3>Step 7: Outputs</h3>
        <ul>
          <li><strong>CLI</strong> — Default: summary and trend stats at the end.</li>
          <li><strong>JSON/CSV:</strong> <code>k6 run --out json=results.json script.js</code>.</li>
          <li><strong>InfluxDB:</strong> <code>k6 run --out influxdb=http://localhost:8086/k6 script.js</code>.</li>
          <li><strong>Grafana Cloud:</strong> Use <code>--out cloud</code> with a project token.</li>
        </ul>

        <h3>Step 8: CI (e.g. GitHub Actions)</h3>
        <p>Run k6 in a job; fail the job if thresholds fail:</p>
        <pre><code>k6 run --threshold "http_req_failed&lt;0.01" script.js</code></pre>

        <h3>How to pass parameters in k6</h3>
        <ul>
          <li><strong>Query parameters:</strong> Build the URL with <code>params</code> or append to path: <code>http.get(url + '?page=1&amp;size=10')</code> or use <code>http.get(url, params)</code> where <code>params</code> holds query keys (e.g. page, size). Use <code>__ENV.USER_ID</code> for variables.</li>
          <li><strong>Path parameters:</strong> Build the path in code: <code>const url = __ENV.BASE_URL + '/users/' + userId + '/orders'</code>. Get <code>userId</code> from <code>__ENV</code>, data array, or random.</li>
          <li><strong>Body (JSON):</strong> Use <code>JSON.stringify()</code> for the body and pass a headers object with <code>Content-Type: application/json</code> in the third argument of <code>http.post()</code>. Use variables from <code>__ENV</code> or shared data.</li>
          <li><strong>Data-driven:</strong> Use <code>SharedArray</code> from <code>k6/data</code> to load JSON/CSV, or pass data via <code>__ENV</code> and split in script. Use <code>data[i].field</code> in the default function.</li>
        </ul>

        <h3>How to pass token in k6</h3>
        <ul>
          <li><strong>Bearer token from environment:</strong> Pass token when running: <code>k6 run -e TOKEN=your_token script.js</code>. In script set the <code>Authorization</code> header to <code>Bearer </code> + <code>__ENV.TOKEN</code> and pass to <code>http.get(url, params)</code>.</li>
          <li><strong>API key in header:</strong> Set header <code>X-API-Key</code> to <code>__ENV.API_KEY</code>. Run with <code>k6 run -e API_KEY=xxx script.js</code>.</li>
          <li><strong>Token from a previous request (login):</strong> Call <code>http.post(loginUrl, body, params)</code>, then <code>loginRes.json('access_token')</code> to get the token. Pass it in the <code>Authorization</code> header for the next <code>http.get()</code>. Do this inside the default function so each VU can get its own token if needed.</li>
          <li><strong>Shared token (one login for all VUs):</strong> Use <code>setup()</code> to run once, get token, and return it; then in <code>default function (data)</code> use <code>data.token</code> in headers.</li>
        </ul>

        <h3>Performance: what k6 gives you</h3>
        <p>k6 reports <strong>http_req_duration</strong> (response time; use p95, p99 for performance), <strong>http_req_failed</strong> (error rate), <strong>http_reqs</strong> (throughput — requests per second), <strong>iterations</strong>, and <strong>vus</strong>. Use <strong>thresholds</strong> to pass/fail the run (e.g. p95 &lt; 500ms, error rate &lt; 1%). <strong>sleep()</strong> adds think time so load is realistic and you don’t max out the load generator.</p>

        <h3>Performance scenarios in k6</h3>

        <h4>Scenario 1: Load test (steady load)</h4>
        <p><strong>Goal:</strong> Validate performance at target load (e.g. 50 VUs for 5 minutes).</p>
        <pre><code>{`export const options = {
  stages: [
    { duration: '1m', target: 50 },   // ramp-up
    { duration: '5m', target: 50 },   // steady
    { duration: '30s', target: 0 },   // ramp-down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],
    http_req_failed: ['rate<0.01'],
  },
};`}</code></pre>

        <h4>Scenario 2: Stress test (increase until failure)</h4>
        <p><strong>Goal:</strong> Ramp up VUs until the system degrades; find capacity.</p>
        <pre><code>{`export const options = {
  stages: [
    { duration: '2m', target: 50 },
    { duration: '2m', target: 100 },
    { duration: '2m', target: 200 },
    { duration: '2m', target: 0 },
  ],
};`}</code></pre>
        <p>Watch when <code>http_req_duration</code> or <code>http_req_failed</code> spikes; that indicates the limit.</p>

        <h4>Scenario 3: Spike test</h4>
        <p><strong>Goal:</strong> Sudden spike in traffic (e.g. flash sale).</p>
        <pre><code>{`export const options = {
  stages: [
    { duration: '10s', target: 10 },
    { duration: '30s', target: 200 },  // fast ramp to 200 VUs
    { duration: '1m', target: 200 },
    { duration: '20s', target: 0 },
  ],
};`}</code></pre>

        <h4>Scenario 4: Endurance / soak test</h4>
        <p><strong>Goal:</strong> Run at moderate load for hours to find leaks or degradation.</p>
        <pre><code>{`export const options = {
  stages: [
    { duration: '5m', target: 30 },
    { duration: '4h', target: 30 },   // hold 30 VUs for 4 hours
    { duration: '5m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(99)<1000'],
    http_req_failed: ['rate<0.01'],
  },
};`}</code></pre>

        <h4>Scenario 5: API load with JSON and custom metrics</h4>
        <p><strong>Goal:</strong> POST JSON, set headers, and track custom metrics (e.g. response size).</p>
        <pre><code>{`import http from 'k6/http';
import { check } from 'k6';
import { Trend } from 'k6/metrics';

const myTrend = new Trend('my_api_duration');

export default function () {
  const payload = JSON.stringify({ key: 'value' });
  const params = {
    headers: { 'Content-Type': 'application/json', 'Authorization': 'Bearer ' + __ENV.TOKEN },
  };
  const res = http.post(__ENV.BASE_URL + '/api/endpoint', payload, params);
  check(res, { 'status 200': (r) => r.status === 200 });
  myTrend.add(res.timings.duration);
}`}</code></pre>

        <h3>Clarifications for performance</h3>
        <ul>
          <li><strong>VUs</strong> = concurrent virtual users. More VUs = more load (if the server and load generator can handle it).</li>
          <li><strong>Stages</strong> = load profile over time. Define ramp-up, steady, and ramp-down to match your scenario.</li>
          <li><strong>Thresholds</strong> = performance gates. If any fail, k6 exits with non-zero code (use in CI to fail the build).</li>
          <li><strong>sleep()</strong> = think time between actions. Without it, each VU runs as fast as possible (high RPS, often unrealistic).</li>
          <li><strong>http_reqs</strong> = total requests; divide by test duration for approximate RPS. Use for throughput targets.</li>
        </ul>

        <h3>AI prompts and using AI with k6</h3>
        <p>k6 has no built-in AI. Use <strong>ChatGPT</strong>, <strong>Claude</strong>, or similar to generate or modify k6 scripts. Copy the prompts below and paste them into the AI (replace [paste] with your content where needed); the AI can output full scripts with stages, thresholds, and checks.</p>

        <h4>Copy-paste prompts (k6)</h4>

        <p class="prompt-label"><strong>1. Load test script</strong></p>
        <pre class="prompt-copy"><code>Write a k6 script that load tests GET https://api.example.com/items with 50 VUs for 5 minutes. Include 1s think time (sleep), ramp-up 1m and ramp-down 30s. Add thresholds: p95 &lt; 500ms, error rate &lt; 1%.</code></pre>

        <p class="prompt-label"><strong>2. Stress test with stages</strong></p>
        <pre class="prompt-copy"><code>k6 script with stages: 0 to 50 VUs in 1m, hold 50 for 2m, then 50 to 150 in 2m, hold 150 for 2m, ramp down to 0. One GET request per iteration with sleep(1). Add thresholds for http_req_duration p95 and http_req_failed.</code></pre>

        <p class="prompt-label"><strong>3. POST with JSON and headers</strong></p>
        <pre class="prompt-copy"><code>k6 script: POST JSON body to /api/orders with Content-Type and Authorization Bearer token. Use __ENV.BASE_URL and __ENV.TOKEN. Add check for status 201 and threshold for p95 &lt; 800ms.</code></pre>

        <p class="prompt-label"><strong>4. From OpenAPI/Swagger</strong></p>
        <pre class="prompt-copy"><code>I have this OpenAPI spec: [paste]. Generate a k6 script that calls the main GET and POST endpoints with 10 VUs for 1 minute, with checks and thresholds.</code></pre>

        <p class="prompt-label"><strong>5. From Postman collection</strong></p>
        <pre class="prompt-copy"><code>Convert this Postman collection export (v2.1) to a k6 script: [paste JSON]. Use stages 0→20→20 VUs, 2m total, with thresholds.</code></pre>

        <p class="prompt-label"><strong>6. Custom metric</strong></p>
        <pre class="prompt-copy"><code>In k6, add a custom Trend metric for response time of a specific request and pass it in thresholds.</code></pre>

        <p class="prompt-label"><strong>7. CI failure message</strong></p>
        <pre class="prompt-copy"><code>My k6 run failed with 'threshold http_req_failed has failed'. What does this mean and how do I fix it (check server, increase threshold, or fix assertions)?</code></pre>

        <h4>How to use the output</h4>
        <ul>
          <li>Save the AI output as <code>script.js</code> (or <code>.mjs</code>). Run with <code>k6 run script.js</code>.</li>
          <li>Replace placeholder URLs and env vars (<code>__ENV.BASE_URL</code>, <code>__ENV.TOKEN</code>) with your values or set them when running: <code>k6 run -e BASE_URL=https://... -e TOKEN=xxx script.js</code>.</li>
          <li>Verify <strong>stages</strong> and <strong>thresholds</strong> match your goal (load vs stress vs spike).</li>
          <li>Run a short test (e.g. 10s) first; then increase duration or VUs.</li>
        </ul>

        <h3>Best practices</h3>
        <ul>
          <li>Use <code>sleep()</code> to avoid hammering the server with no think time.</li>
          <li>Define thresholds for every important metric (latency, error rate).</li>
          <li>Keep scripts in version control and reuse in CI.</li>
          <li>Use environment variables or <code>--env</code> for base URLs and secrets.</li>
        </ul>
      </article>
    </div>

    <!-- Postman panel -->
    <div id="panel-postman" class="tab-panel" role="tabpanel" aria-labelledby="tab-postman" hidden>
      <article class="tool-content">
        <h2>Postman</h2>
        <p>API client and test designer. Postman is <strong>not a dedicated load tool</strong>: it runs requests sequentially (or with limited concurrency in the Runner). Use it for <strong>functional</strong> checks, <strong>smoke</strong> runs, and <strong>baseline</strong> response-time checks. For real performance testing (load, stress, spike, endurance), use <strong>JMeter</strong> or <strong>k6</strong>.</p>

        <h3>Prerequisites</h3>
        <ul>
          <li>Postman desktop app or account for web. For CLI: Node.js for Newman.</li>
        </ul>

        <h3>Step 1: Install Postman</h3>
        <ul>
          <li>Download from <a href="https://www.postman.com/downloads/" target="_blank" rel="noopener">postman.com/downloads</a> (Windows, Mac, Linux).</li>
          <li>Or use Postman for the web (browser).</li>
        </ul>

        <h3>Step 2: Key concepts</h3>
        <ul>
          <li><strong>Request</strong> — Method, URL, headers, body (e.g. JSON).</li>
          <li><strong>Tests</strong> — Scripts (JavaScript) that run after the response (e.g. <code>pm.test("status 200", () => pm.response.to.have.status(200));</code>).</li>
          <li><strong>Collection</strong> — Folder of requests (and optionally tests).</li>
          <li><strong>Environment</strong> — Variables (e.g. <code>baseUrl</code>, <code>apiKey</code>) per environment.</li>
          <li><strong>Collection Runner</strong> — Run a collection many times (iterations) with optional data file.</li>
          <li><strong>Newman</strong> — CLI to run collections (no GUI).</li>
        </ul>

        <h3>Step 3: Create a request and add tests</h3>
        <ol>
          <li>New Request → set method (GET) and URL (e.g. <code>https://httpbin.org/get</code>).</li>
          <li>Click <strong>Send</strong> to verify.</li>
          <li>Open the <strong>Tests</strong> tab and add: <code>pm.test("status is 200", () => pm.response.to.have.status(200));</code></li>
          <li>Send again; the Test Results panel will show pass/fail.</li>
        </ol>

        <h3>Step 4: Create a collection</h3>
        <ol>
          <li>Create a new Collection (e.g. “Performance APIs”).</li>
          <li>Add your request(s) to the collection (drag or save into it).</li>
          <li>Optionally add an <strong>Environment</strong> with variable <code>baseUrl</code> and use <code>{'{{baseUrl}}'}/get</code> in the request URL.</li>
        </ol>

        <h3>Step 5: Run with Collection Runner</h3>
        <ol>
          <li>Click the collection → <strong>Run</strong>.</li>
          <li>Select the requests to run, set <strong>Iterations</strong> (e.g. 100) and optional <strong>Delay</strong> between requests.</li>
          <li>Optional: upload a <strong>Data file</strong> (CSV/JSON) and use variables like <code>{'{{username}}'}</code> in requests.</li>
          <li>Click <strong>Run</strong>; review the run summary (pass/fail, timing).</li>
        </ol>

        <h3>Step 6: Install and run Newman (CLI)</h3>
        <ol>
          <li>Export the collection: Collection → … → Export → Collection v2.1 → save as <code>collection.json</code>.</li>
          <li>Optional: Export environment as <code>env.json</code>.</li>
          <li>Install Newman: <code>npm install -g newman</code>.</li>
          <li>Run: <code>newman run collection.json -n 50 -e env.json</code> (<code>-n</code> = iterations, <code>-e</code> = environment).</li>
        </ol>

        <h3>Step 7: Newman options for light load</h3>
        <pre><code>newman run collection.json -n 100 --delay-request 200 -r cli,json --reporter-json-export results.json</code></pre>
        <ul>
          <li><code>-n 100</code> — 100 iterations (sequential by default).</li>
          <li><code>--delay-request 200</code> — 200 ms between requests.</li>
          <li><code>-r cli,json</code> — CLI output plus JSON report.</li>
        </ul>

        <h3>How to pass parameters in Postman</h3>
        <ul>
          <li><strong>Query parameters:</strong> In the request URL bar, add after <code>?</code> (e.g. <code>{'{{baseUrl}}'}/users?page=1&amp;size=10</code>) or use the <strong>Params</strong> tab: key <code>page</code>, value <code>1</code> or <code>{'{{page}}'}</code>. Use <code>{'{{variableName}}'}</code> for environment or collection variables.</li>
          <li><strong>Path parameters:</strong> In URL use <code>{'{{baseUrl}}'}/users/{'{{userId}}'}/orders</code>. Set <code>userId</code> in the environment, in a pre-request script, or in the Collection Runner data file.</li>
          <li><strong>Body (JSON):</strong> Select <strong>Body</strong> → raw → JSON. Use variables like <code>{'{{name}}'}</code> and <code>{'{{email}}'}</code> in the JSON; they are replaced from the active environment or data file.</li>
          <li><strong>Data-driven (Collection Runner):</strong> Upload a CSV or JSON file in the Runner. Columns/keys become variables. In the request use <code>{'{{username}}'}</code>, <code>{'{{password}}'}</code> in URL, headers, or body. Each iteration gets the next row.</li>
        </ul>

        <h3>How to pass token in Postman</h3>
        <ul>
          <li><strong>Bearer token in header:</strong> In the request, open <strong>Headers</strong>. Add: Key <code>Authorization</code>, Value <code>Bearer {'{{token}}'}</code>. Create an environment variable <code>token</code> and set it (or use a collection variable). For Newman: <code>newman run collection.json -e env.json</code> where <code>env.json</code> contains <code>token</code>.</li>
          <li><strong>API key in header:</strong> Add header <code>X-API-Key</code> = <code>{'{{apiKey}}'}</code> and set <code>apiKey</code> in the environment.</li>
          <li><strong>Token from a previous request (login):</strong> Add a request that POSTs to <code>/auth/login</code>. In its <strong>Tests</strong> tab, parse the response and save the token: <code>const json = pm.response.json(); pm.environment.set('token', json.access_token);</code>. Run the login request first (or order requests in the collection); later requests will use <code>Bearer {'{{token}}'}</code> automatically.</li>
          <li><strong>Pre-request script to get token:</strong> If every request needs a fresh token, add a <strong>Pre-request Script</strong> that calls <code>pm.sendRequest</code> to POST to your auth endpoint, then <code>pm.environment.set('token', ...)</code> from the response. Then the request’s header <code>Authorization: Bearer {'{{token}}'}</code> uses that value.</li>
        </ul>

        <h3>Performance: what Postman/Newman actually give you</h3>
        <ul>
          <li><strong>Response time per request</strong> — Collection Runner and Newman show average response time and min/max for the run. Useful for a quick “is the API slow?” check, not for load.</li>
          <li><strong>No real concurrency</strong> — By default, Newman runs one request after another (or one iteration after another). You are not simulating 100 users at once; you are 1 “user” doing 100 iterations. So you do <strong>not</strong> get true throughput (RPS under concurrent load) or server behavior under stress.</li>
          <li><strong>No percentiles or load profile</strong> — No p95/p99, no ramp-up/stages. For performance criteria (e.g. “p95 &lt; 500ms under 50 users”), use k6 or JMeter.</li>
        </ul>

        <h3>Performance scenarios in Postman (when it’s enough)</h3>

        <h4>Scenario 1: Smoke / sanity (performance angle)</h4>
        <p><strong>Goal:</strong> After a deploy, run the main API requests a few times and check they respond and are not obviously slow.</p>
        <ul>
          <li>Run collection with 5–10 iterations, optional delay. Check that all tests pass and average response time is in a reasonable range (e.g. &lt; 2s).</li>
        </ul>

        <h4>Scenario 2: Baseline response time (single-user)</h4>
        <p><strong>Goal:</strong> Record “normal” response time for key endpoints with no load (e.g. for later comparison with JMeter/k6 under load).</p>
        <ul>
          <li>Run collection with 20–50 iterations, small delay (e.g. 500ms). Note average and max response time. This is a baseline, not a load test.</li>
        </ul>

        <h4>Scenario 3: CI API check (not load)</h4>
        <p><strong>Goal:</strong> In CI, run Newman to verify APIs respond correctly and within a simple time limit (e.g. &lt; 5s).</p>
        <ul>
          <li>Add a test: <code>pm.expect(pm.response.responseTime).to.be.below(5000);</code> and run with low iterations. This is functional + simple perf check, not load testing.</li>
        </ul>

        <h3>Clarifications for performance</h3>
        <ul>
          <li><strong>Iterations</strong> = how many times the collection (or selected requests) runs. Sequential by default; not the same as “N concurrent users.”</li>
          <li><strong>Delay</strong> = pause between requests. Adds realism and avoids hammering the server in smoke runs; does not create load.</li>
          <li><strong>Newman</strong> = same execution model as the Runner: one thread, sequential (unless you use parallel collection runs yourself, which is not standard). For concurrent load, use k6 or JMeter.</li>
          <li>When you need <strong>throughput (RPS), percentiles (p95), or stress/spike tests</strong>, design the same flows in k6 or JMeter and run them there.</li>
        </ul>

        <h3>When to use Postman vs JMeter/k6</h3>
        <table class="tool-table">
          <thead>
            <tr><th>Use Postman/Newman when…</th><th>Use JMeter/k6 when…</th></tr>
          </thead>
          <tbody>
            <tr><td>Validating API behavior and contracts</td><td>Simulating high concurrency (hundreds/thousands of VUs)</td></tr>
            <tr><td>Quick smoke and regression runs</td><td>Precise RPS, ramp-up, and duration control</td></tr>
            <tr><td>Team already uses Postman for development</td><td>Need rich metrics (percentiles, throughput) and thresholds</td></tr>
            <tr><td>Low iteration counts (e.g. &lt; 100)</td><td>Load, stress, or endurance tests</td></tr>
          </tbody>
        </table>

        <h3>AI prompts and using AI with Postman</h3>
        <p>Use <strong>Postman’s built-in AI</strong> (if available in your plan: e.g. “Generate request” or “Generate tests” in the request builder) to create requests or test scripts from a short description. Copy the prompts below and paste into <strong>ChatGPT</strong> or <strong>Claude</strong>; then paste the result into Postman (Tests tab, Pre-request Script, or new request).</p>

        <h4>Copy-paste prompts (Postman)</h4>

        <p class="prompt-label"><strong>1. Single request</strong></p>
        <pre class="prompt-copy"><code>{'Write a Postman GET request URL and headers for a REST API that returns JSON. Base URL is https://api.example.com, path /users, need header Authorization: Bearer token. Use Postman variable {{baseUrl}} and {{token}}.'}</code></pre>

        <p class="prompt-label"><strong>2. Test scripts (assertions)</strong></p>
        <pre class="prompt-copy"><code>Write Postman Tests (JavaScript) for this request: assert status 200, assert response body has a field 'id', assert response time is under 2 seconds. Use pm.test and pm.expect.</code></pre>

        <p class="prompt-label"><strong>3. Collection from list of endpoints</strong></p>
        <pre class="prompt-copy"><code>{'I need a Postman collection with these endpoints: GET /users, GET /users/:id, POST /users with JSON body, PUT /users/:id. For each, give me the method, URL with {{baseUrl}}, and a basic test for status 200 or 201.'}</code></pre>

        <p class="prompt-label"><strong>4. Environment variables</strong></p>
        <pre class="prompt-copy"><code>{'List Postman environment variables I should create for a typical API: baseUrl, token, apiKey. Show how to reference them in the request (e.g. {{baseUrl}}/users).'}</code></pre>

        <p class="prompt-label"><strong>5. Pre-request script (e.g. token)</strong></p>
        <pre class="prompt-copy"><code>Write a Postman pre-request script that gets a Bearer token from POST /auth (body: username, password) and sets pm.environment.set('token', response body token). Use pm.sendRequest.</code></pre>

        <p class="prompt-label"><strong>6. Data-driven run</strong></p>
        <pre class="prompt-copy"><code>{'I have a CSV with columns username, password. How do I use this in Postman Collection Runner and reference {{username}} and {{password}} in the request body?'}</code></pre>

        <p class="prompt-label"><strong>7. Convert to k6</strong></p>
        <pre class="prompt-copy"><code>I have this Postman request (method, URL, headers, body). Convert it to a k6 script with one VU and 10 iterations, with check for status 200.</code></pre>

        <p class="prompt-label"><strong>8. Convert to JMeter</strong></p>
        <pre class="prompt-copy"><code>Convert to JMeter: one Thread Group with 5 threads, one HTTP Request with the same URL and headers as my Postman request.</code></pre>

        <p class="prompt-label"><strong>9. Newman command</strong></p>
        <pre class="prompt-copy"><code>Give me the Newman command to run collection.json 50 times with environment env.json and delay 500ms between requests, output JSON report to results.json.</code></pre>

        <h4>How to use the output</h4>
        <ul>
          <li><strong>In Postman:</strong> Paste generated Tests or Pre-request Script into the request’s Tests / Pre-request tab. For new requests, create the request and then paste URL, headers, or body as needed.</li>
          <li><strong>Postman AI (in-app):</strong> Use “Generate” or “Generate tests” in the request UI; refine the prompt (e.g. “add assertion for response time &lt; 1s”) and re-run if supported.</li>
          <li><strong>Collections:</strong> If the AI outputs multiple requests, create each in Postman and group in a collection; use environment variables for baseUrl and secrets.</li>
          <li>For <strong>load testing</strong>, use the AI to generate a <strong>k6 or JMeter</strong> version of the same flow and run load there; keep Postman for functional and smoke checks.</li>
        </ul>

        <h3>Best practices</h3>
        <ul>
          <li>Use environments for base URL and keys; avoid hardcoding.</li>
          <li>Write clear test names and assertions so failures are easy to understand.</li>
          <li>For real load tests, script the same flows in k6 or JMeter and run there.</li>
          <li>Use Newman in CI for API health and contract checks; keep iterations low unless you need load.</li>
        </ul>
      </article>
    </div>
  </div>
</Layout>

<style>
  .tools-header { margin-bottom: 2rem; }
  .tools-title {
    font-size: 2rem;
    font-weight: 700;
    margin: 0 0 0.5rem;
    letter-spacing: -0.02em;
  }
  .tools-desc { color: var(--text-soft); margin: 0; }
  .tabs { margin-top: 0.5rem; }
  .tab-list {
    display: flex;
    gap: 0.25rem;
    margin-bottom: 1.5rem;
    border-bottom: 1px solid var(--border);
    padding-bottom: 0;
  }
  .tab-btn {
    background: none;
    border: none;
    border-bottom: 2px solid transparent;
    color: var(--muted);
    cursor: pointer;
    font: inherit;
    font-size: 0.95rem;
    padding: 0.75rem 1.25rem;
    margin-bottom: -1px;
    transition: color 0.15s ease;
  }
  .tab-btn:hover { color: var(--text-soft); }
  .tab-btn.active {
    color: var(--accent);
    border-bottom-color: var(--accent);
  }
  .tab-panel { display: block; }
  .tab-panel[hidden] { display: none !important; }
  .tool-content {
    max-width: 720px;
    color: var(--text-soft);
  }
  .tool-content h2 { font-size: 1.5rem; margin-top: 0; color: var(--text); }
  .tool-content h3 { font-size: 1.15rem; margin-top: 1.5rem; color: var(--text); }
  .tool-content h4 { font-size: 1rem; margin-top: 1.25rem; color: var(--muted); font-weight: 600; }
  .tool-content ul, .tool-content ol { margin: 0.5rem 0 1rem 1.25rem; }
  .tool-content li { margin: 0.35rem 0; }
  .tool-content pre {
    margin: 0.75rem 0 1rem;
    border: 1px solid var(--border);
    border-radius: var(--radius);
  }
  .prompt-label { margin: 1rem 0 0.35rem; font-size: 0.95rem; color: var(--text); }
  .prompt-copy {
    margin: 0 0 0.5rem;
    padding: 0.85rem 1.1rem;
    border-radius: var(--radius);
    border: 1px solid var(--border);
    background: var(--bg-elevated);
    user-select: all;
    cursor: text;
  }
  .prompt-copy code { font-size: 0.9rem; white-space: pre-wrap; word-break: break-word; color: var(--text-soft); }
  .tool-table {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
    border-radius: var(--radius);
    overflow: hidden;
    border: 1px solid var(--border);
  }
  .tool-table th, .tool-table td {
    border: none;
    padding: 0.65rem 0.85rem;
    text-align: left;
  }
  .tool-table th { background: var(--surface); color: var(--muted); font-weight: 600; }
  .tool-table tr:not(:last-child) td { border-bottom: 1px solid var(--border); }
</style>

<script>
  document.querySelector('#tools-tabs')?.addEventListener('click', (e) => {
    const btn = e.target.closest('[data-tab]');
    if (!btn) return;
    const tabId = btn.getAttribute('data-tab');
    const panelId = 'panel-' + tabId;
    document.querySelectorAll('.tab-btn').forEach((b) => {
      b.classList.remove('active');
      b.setAttribute('aria-selected', 'false');
    });
    document.querySelectorAll('.tab-panel').forEach((p) => {
      p.classList.remove('active');
      p.setAttribute('hidden', '');
    });
    btn.classList.add('active');
    btn.setAttribute('aria-selected', 'true');
    const panel = document.getElementById(panelId);
    if (panel) {
      panel.classList.add('active');
      panel.removeAttribute('hidden');
    }
  });
</script>
